{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade56d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee380f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"../texts/deephaven.txt\").read()\n",
    "sentences = sent_tokenize(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36171427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GPT2 XL model\n",
    "model = GPT2Model.from_pretrained('gpt2-xl', \n",
    "                                  low_cpu_mem_usage=True,\n",
    "                                  output_hidden_states=True)\n",
    "tok = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "# end of sentence/text token padding\n",
    "tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb25fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentence):\n",
    "    inp_tok = tok(sentence,\n",
    "             padding=True,\n",
    "             return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    input_ids = inp_tok[\"input_ids\"]\n",
    "    output = model(input_ids)\n",
    "\n",
    "    # return tokenized text for indexing\n",
    "    tokenized_text = [tok.decode(id).strip() for id in input_ids[0]]\n",
    "\n",
    "    # extract hidden states\n",
    "    embs = torch.stack(output['hidden_states'], dim=0)\n",
    "    embs = torch.squeeze(embs, dim=1)\n",
    "    embs = embs.permute(1,0,2)\n",
    "\n",
    "    # mean embeddings in the last four layers\n",
    "    vectors = [torch.mean(t[-4:], dim=0).detach().numpy() for t in embs]\n",
    "    \n",
    "    return tokenized_text, input_ids, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ccbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "context_embeddings = []\n",
    "context_tokens = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized_text, ids, list_token_embeddings = get_sentence_embeddings(sentence)\n",
    "    tokens = OrderedDict()\n",
    "    for token in tokenized_text[1:-1]:\n",
    "        if token in tokens:\n",
    "            tokens[token] += 1\n",
    "        else:\n",
    "            tokens[token] = 1\n",
    "        token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
    "        current_index = token_indices[tokens[token]-1]\n",
    "        token_vec = list_token_embeddings[current_index]\n",
    "        context_tokens.append(token)\n",
    "        context_embeddings.append(token_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d363698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "def get_neighbors(word,unique=False,k=50):\n",
    "    word_list = []\n",
    "    nn = NearestNeighbors(n_neighbors = k, \n",
    "                            algorithm = 'ball_tree').fit(context_embeddings)\n",
    "    if word in context_tokens:\n",
    "        w_idx = context_tokens.index(word)\n",
    "        d, idx = nn.kneighbors([context_embeddings[w_idx]])\n",
    "        for d, idx in zip(d[0],idx[0]):\n",
    "            if unique and context_tokens[idx] in word_list:\n",
    "                next\n",
    "            else:\n",
    "                print(np.round(d,3),context_tokens[idx])\n",
    "            word_list.append(context_tokens[idx])\n",
    "    else:\n",
    "        print(\"error: {0} not in vocab\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32455d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"elapsed time: %s (seconds)\" % np.round((time.time() - start_time),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b982b347",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 village\n",
      "200.748 village\n",
      "221.168 village\n",
      "228.722 garden\n",
      "237.215 village\n",
      "252.319 sunset\n",
      "253.323 ures\n",
      "253.734 farm\n",
      "255.051 age\n",
      "256.784 Parish\n",
      "257.324 garden\n",
      "260.977 oor\n",
      "261.124 farm\n",
      "261.276 pasture\n",
      "261.503 farm\n",
      "263.217 Parish\n",
      "264.119 village\n",
      "264.297 river\n",
      "264.413 town\n",
      "266.214 York\n",
      "266.983 country\n",
      "267.381 country\n",
      "267.814 country\n",
      "267.86 river\n",
      "268.167 village\n"
     ]
    }
   ],
   "source": [
    "get_neighbors(\"village\",k=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7039806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 context_tokens.index(<span style=\"color: #808000; text-decoration-color: #808000\">\"Schooner\"</span>)                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Schooner'</span> is not in list\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m                                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 context_tokens.index(\u001b[33m\"\u001b[0m\u001b[33mSchooner\u001b[0m\u001b[33m\"\u001b[0m)                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0m\u001b[32m'Schooner'\u001b[0m is not in list\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_tokens.index(\"schooner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5addbbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
