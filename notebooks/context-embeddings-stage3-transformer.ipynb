{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade56d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee380f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"../texts/deephaven.txt\").read()\n",
    "sentences = sent_tokenize(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36171427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load GPT2 XL model\n",
    "model = GPT2Model.from_pretrained('gpt2-xl', \n",
    "                                  low_cpu_mem_usage=True,\n",
    "                                  output_hidden_states=True)\n",
    "tok = AutoTokenizer.from_pretrained(\"gpt2-xl\")\n",
    "\n",
    "# end of sentence/text token padding\n",
    "tok.pad_token = tok.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb25fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeddings(sentence):\n",
    "    inp_tok = tok(sentence,\n",
    "             padding=True,\n",
    "             return_tensors=\"pt\").to(next(model.parameters()).device)\n",
    "    input_ids = inp_tok[\"input_ids\"]\n",
    "    output = model(input_ids)\n",
    "\n",
    "    # return tokenized text for indexing\n",
    "    tokenized_text = [tok.decode(id).strip() for id in input_ids[0]]\n",
    "\n",
    "    # extract hidden states\n",
    "    embs = torch.stack(output['hidden_states'], dim=0)\n",
    "    embs = torch.squeeze(embs, dim=1)\n",
    "    embs = embs.permute(1,0,2)\n",
    "\n",
    "    # mean embeddings in the last four layers\n",
    "    vectors = [torch.mean(t[-4:], dim=0).detach().numpy() for t in embs]\n",
    "    \n",
    "    return tokenized_text, input_ids, vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ccbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "context_embeddings = []\n",
    "context_tokens = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized_text, ids, list_token_embeddings = get_sentence_embeddings(sentence)\n",
    "    tokens = OrderedDict()\n",
    "    for token in tokenized_text[1:-1]:\n",
    "        if token in tokens:\n",
    "            tokens[token] += 1\n",
    "        else:\n",
    "            tokens[token] = 1\n",
    "        token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
    "        current_index = token_indices[tokens[token]-1]\n",
    "        token_vec = list_token_embeddings[current_index]\n",
    "        context_tokens.append(token)\n",
    "        context_embeddings.append(token_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d363698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "def get_neighbors(word,unique=False,k=50):\n",
    "    word_list = []\n",
    "    nn = NearestNeighbors(n_neighbors = k, \n",
    "                            algorithm = 'ball_tree').fit(context_embeddings)\n",
    "    if word in context_tokens:\n",
    "        w_idx = context_tokens.index(word)\n",
    "        d, idx = nn.kneighbors([context_embeddings[w_idx]])\n",
    "        for d, idx in zip(d[0],idx[0]):\n",
    "            if unique and context_tokens[idx] in word_list:\n",
    "                next\n",
    "            else:\n",
    "                print(np.round(d,3),context_tokens[idx])\n",
    "            word_list.append(context_tokens[idx])\n",
    "    else:\n",
    "        print(\"error: {0} not in vocab\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b982b347",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 town\n",
      "251.413 house\n",
      "253.665 town\n",
      "262.429 town\n",
      "263.421 town\n",
      "264.579 town\n",
      "264.906 town\n",
      "266.347 haven\n",
      "267.849 city\n",
      "268.51 town\n",
      "270.408 town\n",
      "270.582 there\n",
      "272.044 town\n",
      "273.946 town\n",
      "274.142 ills\n",
      "274.684 warehouses\n",
      "277.041 town\n",
      "279.452 country\n",
      "279.58 village\n",
      "280.314 nature\n",
      "280.561 parish\n",
      "280.696 host\n",
      "281.562 sea\n",
      "281.577 place\n",
      "281.822 town\n"
     ]
    }
   ],
   "source": [
    "get_neighbors(\"town\",k=25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
