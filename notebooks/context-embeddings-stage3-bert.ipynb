{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190b6c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "\n",
    "# load BERT tokenizer\n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1240c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855bc4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased',\n",
    "           output_hidden_states = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf3af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"../texts/deephaven.txt\").read()\n",
    "sentences = sent_tokenize(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a028a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text):\n",
    "    marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "    # trim tokens, if needed\n",
    "    tokenized_text = tok.tokenize(marked_text)[:512]\n",
    "    indexed_tokens = tok.convert_tokens_to_ids(tokenized_text)\n",
    "    segments_ids = [1]*len(indexed_tokens)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensor = torch.tensor([segments_ids])\n",
    "    return tokenized_text, tokens_tensor, segments_tensor\n",
    "\n",
    "def get_bert_embeddings(tokens_tensor, segments_tensor, model):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(tokens_tensor, segments_tensor)\n",
    "    \n",
    "    hidden_states = outputs[2]\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "    token_embeddings = token_embeddings.permute(1,0,2)\n",
    "    token_vecs_sum = []\n",
    "    for token in token_embeddings:\n",
    "        sum_vec = torch.sum(token[-4:], dim=0).detach().numpy()\n",
    "        token_vecs_sum.append(sum_vec)\n",
    "    return token_vecs_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "context_embeddings = []\n",
    "context_tokens = []\n",
    "\n",
    "for sentence in sentences:\n",
    "    tokenized_text, tokens_tensor, segments_tensors = bert_text_preparation(sentence)\n",
    "    list_token_embeddings = get_bert_embeddings(tokens_tensor, segments_tensors, model)\n",
    "    tokens = OrderedDict()\n",
    "    for token in tokenized_text[1:-1]:\n",
    "        if token in tokens:\n",
    "            tokens[token] += 1\n",
    "        else:\n",
    "            tokens[token] = 1\n",
    "        token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
    "        current_index = token_indices[tokens[token]-1]\n",
    "        token_vec = list_token_embeddings[current_index]\n",
    "        context_tokens.append(token)\n",
    "        context_embeddings.append(token_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef26e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "def get_neighbors(word,unique=False,k=50):\n",
    "    word_list = []\n",
    "    nn = NearestNeighbors(n_neighbors = k, \n",
    "                            algorithm = 'ball_tree').fit(context_embeddings)\n",
    "    if word in context_tokens:\n",
    "        w_idx = context_tokens.index(word)\n",
    "        d, idx = nn.kneighbors([context_embeddings[w_idx]])\n",
    "        for d, idx in zip(d[0],idx[0]):\n",
    "            if unique and context_tokens[idx] in word_list:\n",
    "                next\n",
    "            else:\n",
    "                print(np.round(d,3),context_tokens[idx])\n",
    "            word_list.append(context_tokens[idx])\n",
    "    else:\n",
    "        print(\"error: {0} not in vocab\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c05a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_neighbors(\"schooner\",unique=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5206455",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"elapsed time: %s (seconds)\" % np.round((time.time() - start_time),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11913e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_tokens.index(\"schooner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e570679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
